{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Buzz-Word Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"https://media.giphy.com/media/ckJF143W1gBS8Hk833/giphy.gif\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Great at understanding the sentence structure\n",
    "\n",
    "<br />\n",
    "\n",
    "## Can predict what comes next based on previous inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- \"Language Models (LMs) estimate the relative likelihood of different phrases and are useful in many different Natural Language Processing applications (NLP). For example, they have been used in Twitter Bots for ‘robot’ accounts to form their own sentences.\" - [ref](https://towardsdatascience.com/learning-nlp-language-models-with-real-data-cdff04c51c25)\n",
    "<br />\n",
    "\n",
    "- The goal of probabilistic language modelling is to calculate the probability of a sentence of sequence of words and can be used to find the probability of the next word in the sequence - [ref](https://towardsdatascience.com/learning-nlp-language-models-with-real-data-cdff04c51c25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"https://miro.medium.com/max/4140/1*IFVX74cEe8U5D1GveL1uZA.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Transformers were developed to solve the problem of sequence transduction, or neural machine translation. That means any task that transforms an input sequence to an output sequence. This includes speech recognition, text-to-speech transformation, etc.. - [ref](https://towardsdatascience.com/transformers-141e32e69591)\n",
    "\n",
    "<br />\n",
    "\n",
    "- Neural networks can achieve this same behavior using attention, focusing on part of a subset of the information they are given. For example, an RNN can attend over the output of another RNN. At every time step, it focuses on different positions in the other RNN.\n",
    "To solve these problems, Attention is a technique that is used in a neural network. For RNNs, instead of only encoding the whole sentence in a hidden state, each word has a corresponding hidden state that is passed all the way to the decoding stage. Then, the hidden states are used at each step of the RNN to decode - [ref](https://towardsdatascience.com/transformers-141e32e69591)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>    \n",
    "<img src=\"https://miro.medium.com/max/1400/1*GQzYZuAMWr3lN_IACBfvAA.png\" width=600px>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<h2> GPT-2 </h2>\n",
    "<img src=\"https://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## GPT-2 stands for “Generative Pretrained Transformer 2”:\n",
    "\n",
    "- “Generative” means the model was trained to predict (or “generate”) the next token in a sequence of tokens in an unsupervised way. In other words, the model was thrown a whole lot of raw text data and asked to figure out the statistical features of the text to create more text.\n",
    "- “Pretrained” means OpenAI created a large and powerful language model, which they fine-tuned for specific tasks like machine translation later on. This is kind of like transfer learning with Imagenet, except it’s for NLP. This retraining approach became quite popular in 2018 and is very likely to be a trend that continues throughout 2019.\n",
    "- “Transformer” means OpenAI used the transformer architecture, as opposed to an RNN, LSTM, GRU or any other 3/4 letter acronym you have in mind. I’m not going to discuss the transformer architecture in detail since there’s already another great article on the FloydHub blog that explains how it works. - [ref](https://blog.floydhub.com/gpt2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>    \n",
    "<img src=\"https://paper-attachments.dropbox.com/s_972195A84441142620E4C92312EA63C9665C3A86AFFD1D713034FA568ADFC5F9_1555423661299_GPT2.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's Get Our Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "iqMZsAM5pJR2",
    "outputId": "f3c2d222-aa34-438d-9180-c400de90fe65",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.12.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Vwfhsmx0zH85fVBg2Rcm239qLhqywRBk\n",
      "To: /content/am_corpus_original.txt\n",
      "100%|██████████| 173k/173k [00:00<00:00, 54.8MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-2nsx5NKHoFjfzaDYX-sOksdCIgHBSes\n",
      "To: /content/am_corpus.txt\n",
      "100%|██████████| 328k/328k [00:00<00:00, 72.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "import gdown\n",
    "\n",
    "\n",
    "def get_files_from_gdrive(url: str, fname: str) -> None:\n",
    "    file_id = url.split(\"/\")[5]\n",
    "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    gdown.download(url, fname, quiet=False)\n",
    "\n",
    "\n",
    "get_files_from_gdrive(\n",
    "    \"https://drive.google.com/file/d/1Vwfhsmx0zH85fVBg2Rcm239qLhqywRBk/view?usp=sharing\",\n",
    "    \"am_corpus_original.txt\",\n",
    ")\n",
    "\n",
    "get_files_from_gdrive(\n",
    "    \"https://drive.google.com/file/d/1-2nsx5NKHoFjfzaDYX-sOksdCIgHBSes/view?usp=sharing\",\n",
    "    \"am_corpus.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "KBkpRgBCBS2_",
    "outputId": "082990f7-1b66-49ce-b729-49a12b39a122",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "# !pip install -q gpt-2-simple\n",
    "import gpt_2_simple as gpt2\n",
    "from datetime import datetime\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bj2IJLHP3KwE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPU\n",
    "\n",
    "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
    "\n",
    "You can verify which GPU is active by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "sUmTooTW3osf",
    "outputId": "76bd143a-22c5-474d-f96f-2da985e471b2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 17 12:44:21 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wXB05bPDYxS",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Downloading GPT-2\n",
    "\n",
    "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
    "\n",
    "There are three released sizes of GPT-2:\n",
    "\n",
    "* `124M` (default): the \"small\" model, 500MB on disk.\n",
    "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
    "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
    "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "P8wSlgXoDPCR",
    "outputId": "dca36877-e04b-4d10-a33c-dc2b3960e24c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 587Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 78.3Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 1.03Git/s]                                                   \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:02, 182Mit/s]                                   \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 721Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 147Mit/s]                                                 \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 192Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "gpt2.download_gpt2(model_name=\"124M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8KXuKWzQSsN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mounting Google Drive\n",
    "\n",
    "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
    "\n",
    "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "puq4iC6vUAHc",
    "outputId": "619478d2-d3ad-4c43-960e-c6ad9970bb1b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "gpt2.mount_gdrive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OFnPCLADfll",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "file_name = \"am_corpus.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LdpZQXknFNY3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finetune GPT-2\n",
    "\n",
    "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
    "\n",
    "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
    "\n",
    "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
    "\n",
    "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aeXshJM-Cuaf",
    "outputId": "18188de3-7f7e-4892-a7ee-169bda6388ad",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 81742 tokens\n",
      "Training...\n",
      "[10 | 20.32] loss=3.51 avg=3.51\n",
      "[20 | 32.82] loss=3.78 avg=3.64\n",
      "[30 | 45.33] loss=4.48 avg=3.93\n",
      "[40 | 57.83] loss=4.03 avg=3.95\n",
      "[50 | 70.34] loss=3.00 avg=3.76\n",
      "[60 | 82.83] loss=4.37 avg=3.86\n",
      "[70 | 95.32] loss=3.53 avg=3.81\n",
      "[80 | 107.82] loss=3.52 avg=3.77\n",
      "[90 | 120.33] loss=3.68 avg=3.76\n",
      "[100 | 132.82] loss=3.73 avg=3.76\n",
      "[110 | 145.32] loss=3.05 avg=3.69\n",
      "[120 | 157.82] loss=3.06 avg=3.64\n",
      "[130 | 170.32] loss=2.04 avg=3.51\n",
      "[140 | 182.82] loss=3.83 avg=3.53\n",
      "[150 | 195.31] loss=2.28 avg=3.44\n",
      "[160 | 207.81] loss=1.72 avg=3.33\n",
      "[170 | 220.30] loss=2.04 avg=3.24\n",
      "[180 | 232.79] loss=1.33 avg=3.13\n",
      "[190 | 245.29] loss=1.30 avg=3.02\n",
      "[200 | 257.83] loss=1.56 avg=2.94\n",
      "======== SAMPLE 1 ========\n",
      " followed, and my eyes were burning out, but ive turned down the heat to make this concoction go down well with the, erm, everybody  [verse 2] i watched your little science fiction children's television set today aggravate the boredom by explaining to im friend a you lived in i could have been your Doctor O, your first lady  [chorus] you know that science fiction is for kids i knew that you lived for science fiction it sort of does them a lot and i really like it when you explain to me every waking moment when youve been fucking  i went so long that i barely know how i got you here i thought you might like a bite to eat joe with a twist of fate i thought you might like a glass of water  [verse 3] and then suddenly i woke a sinking feeling i ever wanted to be clean i thought about it for a moment i might be one amongst many i cast a spell on the mirror ill slip and behold, a stranger shapes me back to reality i cast a spell on the mirror ill slip and behold, a stranger shapes me back to reality i cast a spell on the mirror ill slip and behold, a stranger shapes me back to reality i cast a spell on the mirror ill slip and behold, a stranger shapes me back to reality [intro] im pinned down by the well drawn maze an unfamiliar face marks my path im pinned down by the well drawn maze [verse 1] past a time when you kept your cool and stayed out of it all, past the bright lights and time machines, past the booths selling chocolate (oh) and my days wake up without a love like yours  [verse 2] new things a, b, c, d, e, f, g, h, i, k, l, really, really love them all, most dear  [bridge] i was trying to think of a good baseball memory remember when you always looked d - low - fail - d - low - low - high - both came up empty but pretty sure made it that high were pretty sure made it pretty sure not quite out of these three  [chorus] those days were pretty much a blank cheetah caked with smoke and brimstone and stitched to poles really, really like pipe and i bet you just smelt it and smelt it like the bile the day after the cataclysmic end of the race  [verse 3] new things a, b, c, d, e, f, g, h, i, k, l, really, really like dogs, cats, cuddles, balls, and music just wanna sing to you and i know everybodys wherever theyuck wrote that song thinking theyd just summathe why theyre tessin all 1) She woke up naked and smitten with high fiving and her ears decorated with sadist paintings and all the attentionary looks she got her hair done in gruesomely done samsillon style  2) She woke up with a tessicil hat and waved it gracefully at the approaching cadence of the whiz kids in a saturday morning van, all covered in the dirty dream, all the while pretending to be a cuddlin song vixens, theres pictures of you but just dont exist what do you wanna be?  [bridge] got all tessicil hats on, showing off what i know bout the city i live in hows exactly i got obsessed with logic and why did what i had to give last night make it through the day? i havent had a look in forever i havent had the midi songropoulos but if youve ever wondered whyami had what it took to wake me up yesterday then i know it isnt whati in, because it wasse both high noon and i had to be outside in the daylight just happened to be out walking and just happened to be in a saturday morning suitcase full of rave music  [verse 1] out of nowhere, suddenly, something incredible happens. your dreams become reality, your instincts take over, the very frame of reference you wont remember when youre born and youll recall having sex forever changing thoughts and feelings and memories and memories  [verse 2] well, suddenly i woke up screaming, my stomach churning out words i just had to come out and out with a bang and i a stranger has grabbed my arm and i need to flee the room i am held in every bit of mud I havent been in a fire before  and now the stranger is in the courtroom the summathets gone gibberish! oh, how we felt so horribly bruised and we were so bruised and we cried so so i turned to nothing but sand and said \"shall i go?\" it went well, didn’t and now the stranger is in the magistrates court  well, now the stranger is in the magistrates court ill be fiendishly handsome, ill be the fiendishly handsome man, ill be the man just cause he\n",
      "\n",
      "[210 | 282.13] loss=0.68 avg=2.82\n",
      "[220 | 294.64] loss=0.74 avg=2.72\n",
      "[230 | 307.15] loss=0.46 avg=2.61\n",
      "[240 | 319.65] loss=0.43 avg=2.51\n",
      "[250 | 332.14] loss=0.58 avg=2.42\n",
      "[260 | 344.65] loss=0.52 avg=2.34\n",
      "[270 | 357.15] loss=0.80 avg=2.27\n",
      "[280 | 369.66] loss=0.37 avg=2.20\n",
      "[290 | 382.18] loss=0.45 avg=2.13\n",
      "[300 | 394.69] loss=0.29 avg=2.06\n",
      "[310 | 407.19] loss=0.17 avg=1.99\n",
      "[320 | 419.69] loss=0.13 avg=1.92\n",
      "[330 | 432.22] loss=0.14 avg=1.86\n",
      "[340 | 444.71] loss=0.16 avg=1.80\n",
      "[350 | 457.23] loss=0.32 avg=1.75\n",
      "[360 | 469.74] loss=0.08 avg=1.69\n",
      "[370 | 482.24] loss=0.17 avg=1.64\n",
      "[380 | 494.74] loss=0.15 avg=1.60\n",
      "[390 | 507.25] loss=0.12 avg=1.55\n",
      "[400 | 519.75] loss=0.11 avg=1.51\n",
      "======== SAMPLE 1 ========\n",
      " fiction, with its squabbling, its shouting, its coming to the brink of war and its defining moment is as if its own making, its shell clearly visible when its been nearly 3 years and its warm and have you got that big of you? good morning, everyone, my name are you with me? are you with me? on the way back, my name is asyl (sic) and if you wanted to, you could have seen it in his eyes because of the way he looked at me in the car [verse 1] i saw you flicking through a scrapbook and instead i found a scrapbook full of all the sudden wanton murder mysteries  [pre-chorus] from the looks of it it looks like its breaking point and theres actually a small but it doesn’t stop at simple plagiarism, accidental print  [chorus] and the ones that fail are those that are too easy to fall for are those that are hard to fall for are those that are too easy to fall for  [verse 2] break a mirror, im en route to exacting intents from a distance theres no need to stop and just walk away as soon as the intention is fulfilled its nothing more than a starting point and theres a small but it doesn’t stop at simple plagiarism, accidental print  [pre-chorus] i saw you swinging from the night the police arrived at your door swinging bottles, bottles and coughs am i getting too close? its a very real risk, though, to take such a risk alone and to assume that the chance that i may fall are too small  [chorus] and the ones that fail are those that are too easy to fall for are those that are hard to fall for are those that are too easy to fall for  [bridge] those are the ones that are too easy to fall for cant figure out how to fly dinghy, its a pity it took us three days to get to this point still on our own way no indication as to how i am falling short i think it or not thats too easy a bet, but its certainly not too difficult a bet to make  [instrumental break]  [verse 3] the shadow plays a very musical instrument on your tongue as you take to the air, a cappella band playing a reckless abandon on a rainy day its a reckless hitchhiking on a rainy next to the sea i think its pretty damn hard to make it past the rinks without putting on a brave in the food trucks and bars and smoke until you die  [pre-chorus] those people are crazy, but mr. prehistoric, hes a lot bigger now that im even anthropomorphic than shes thinking and thinking and probably having a bit too [verse 1] just got my back pocket when you walked in, youre hard to knock off anyone hes kissing or walking out - just tried so if you would just show me your way, i could take some and just give you some that don, youll be fine  [verse 2] its more a hunger than a thirst hole wake with me when the electricity goes off in my room and the heat is on bring along a new episode and calm nerves before the moments drag you to your own and the jigsaw of what it feels like to be entirely dismembered  [chorus] do the bad thing take off your wedding ring and slap the floor with a rattle baby say, \"do the bad thing\"? and i have no idea do the bad thing themselves or do the worst thing at the party?  [verse 3] as the days drag they become weeks gloom and you can tell by the stares that they've had that must have been awful  [pre-chorus] they're watching over us, their minds blank and their bodies frozen, their imaginations caught up in the process  [chorus] do the bad thing take off your wedding ring and slap the floor with a rattle baby say, \"do the bad thing\"? and i have no idea do the bad thing thing thing thing thing thing thing thing thing thing thing thing thing thing  [outro] maybe have a cigarette and a look in the mirror maybe have a low and the talk is boring as hell maybe have a look at the stars through binoculars and think of a name that i would like to give you  [verse 4] and all fortunate winners will get to wear the crown on their own initiative and be the first to yourself  [pre-chorus] maybe have a cigarette and a look in the mirror maybe have a low and the talk is boring as hell maybe have a look at the stars through binoculars and think of a name that i would like to give you  [bridge] maybe have a cigarette and a look in the mirror maybe have a low and the talk is boring as hell maybe have a low and the talk is boring as hell maybe have a low and the talk are boring as hell, but are you the one who does it? [intro\n",
      "\n",
      "[410 | 543.02] loss=0.08 avg=1.46\n",
      "[420 | 555.52] loss=0.09 avg=1.42\n",
      "[430 | 568.06] loss=0.09 avg=1.39\n",
      "[440 | 580.57] loss=0.06 avg=1.35\n",
      "[450 | 593.08] loss=0.08 avg=1.31\n",
      "[460 | 605.60] loss=0.11 avg=1.28\n",
      "[470 | 618.10] loss=0.08 avg=1.25\n",
      "[480 | 630.61] loss=0.09 avg=1.22\n",
      "[490 | 643.12] loss=0.10 avg=1.19\n",
      "[500 | 655.62] loss=0.07 avg=1.16\n",
      "Saving checkpoint/run1/model-500\n",
      "[510 | 670.47] loss=0.08 avg=1.14\n",
      "[520 | 682.98] loss=0.05 avg=1.11\n",
      "[530 | 695.48] loss=0.07 avg=1.08\n",
      "[540 | 707.98] loss=0.07 avg=1.06\n",
      "[550 | 720.47] loss=0.07 avg=1.04\n",
      "[560 | 732.99] loss=0.05 avg=1.01\n",
      "[570 | 745.50] loss=0.06 avg=0.99\n",
      "[580 | 758.01] loss=0.08 avg=0.97\n",
      "[590 | 770.51] loss=0.08 avg=0.95\n",
      "[600 | 783.02] loss=0.07 avg=0.93\n",
      "======== SAMPLE 1 ========\n",
      " memories were sinking in without her and she couldnt breathe , and she landed hard on her back , and broke her jaw it was while on the run she said \" never come \" [ verse 1 ] \" i to you , \" please , stop it \" - teasing , even when know it ahead lights the house , c - tract . . . \" the week before i sick told , \" you don t know , all i it it ' s me , i got letters from all around the world , from boys who left but who still calls , and who still writes my name , but i never thought would that this could hit the fan like a big rocket , so to speak i never thought i could it fall down the [ ] , a huge in the the of , over there , nothing to it dear , youve got control of the situation ' t come to ' it from us , or from them [ 9 ] you don t , hug , tell don ' t , tangle , tell ' em to put it down to fate i ' d the othere you were lucky , having the stomach to on [ british dub ] [  ] may ever , may ever , if ever it even the have chance [ part 2 ] high above, bells ring at the scene for the searchlight , a . x , a . . . who are the policemen ? \" who never come up to touch , to inquire if required treatment for ; hair right . low , pretty wife low low murmur , so don ' t [ the bridge ] , what do you really know ? \" ah , baby , a dime a roll [ instrumental break ] [ 8 . ] i a metaphor : to you , like me i think i raise my hand if i was you immediate , immediate yes , definitely , no [ , no] [ . ] the searchlight , a . x . , a dime . when the bells again ring , a \" , - sure , \" but the amazing man , that penny a roll [ chorus ] , you cant look into . . . . from your hair to nothing . shiny new . . . shiny new look good on [ commercial ] [ love ] , ever so suspicious , ever so steadfast , steadfast faith so fucking hard you ' re looking at his . i see your , you know itin . youre lookin like i haven ' t found what youve been . you sure just me . dope i see itin brick , whatever the the cause , the he that plagiarized from a book forgot i . co . kane a long way from home [ and ] back home they ' re . a state police after the break , [ a band were filled with ] just might ' just might find everybody . ready to start a new chapter in the happy memory of us all [ ] . you can dream , baby , \" i just wanna finish \" you ’ got permission to be , to be who you gonna be \" baby , wanna be always keep your seatbelt on , okay ? like , \" no , its pop song an even r more pop song than \" yeah , well tune in . \" - - t : , . . . [ instrumental break ] [ 7 . ] i with open eyes . . . like . i freaking missed you . never met . , okay you ' re some nerd with a rockstar on , and , okay , . could have used a hint or two you made \" but , no , it only confused me . . . down , you go fast . like \" fancy taking you \" [  1 ] he got his friends . a . t . and ever hungry black a [ episode  2 ] black as a . , a . snarl was at audible click \" , \" go \" , - how to even you black , snarl \" at quick . i just want to \" go \" , - how to even you black , fi . [ from verse 3 ] got a . guys high ho changed at the airport and while waiting . they us eat .vich food and chat chat . you should probably having that that they had just had they to each got their hair done and gamely puffed . smiled and puffed their chests all the while [  4 ] got in . the . the and the chat . . . like . . . . like ever you . , - ill even you [  5 ] got . the and the chat . . . like ever you . - you know \" we know each other , keepin hands ! how do you feel , little darling ( ) you’ve been a long time , (in time , if you ’ remember )  [ verse  ] just . ever with you , always . we hung out . hand went out the door and touched heart . thought was good while he was in utero . how do you feel , little darling ( ) you’ve been a long time , ( ' mind ) ever since . died before i could breathe . i thought was well she was just tonight ( tonight ) breathe , he really . wanna . . . sleep i thought was pretty\n",
      "\n",
      "[610 | 806.23] loss=0.06 avg=0.91\n",
      "[620 | 818.74] loss=0.05 avg=0.89\n",
      "[630 | 831.25] loss=0.07 avg=0.88\n",
      "[640 | 843.77] loss=0.05 avg=0.86\n",
      "[650 | 856.27] loss=0.05 avg=0.84\n",
      "[660 | 868.77] loss=0.06 avg=0.83\n",
      "[670 | 881.29] loss=0.07 avg=0.81\n",
      "[680 | 893.78] loss=0.06 avg=0.80\n",
      "[690 | 906.27] loss=0.05 avg=0.78\n",
      "[700 | 918.77] loss=0.06 avg=0.77\n",
      "[710 | 931.27] loss=0.04 avg=0.75\n",
      "[720 | 943.77] loss=0.05 avg=0.74\n",
      "[730 | 956.28] loss=0.05 avg=0.73\n",
      "[740 | 968.78] loss=0.05 avg=0.71\n",
      "[750 | 981.27] loss=0.07 avg=0.70\n",
      "[760 | 993.77] loss=0.04 avg=0.69\n",
      "[770 | 1006.29] loss=0.04 avg=0.68\n",
      "[780 | 1018.79] loss=0.06 avg=0.66\n",
      "[790 | 1031.29] loss=0.07 avg=0.65\n",
      "[800 | 1043.81] loss=0.06 avg=0.64\n",
      "======== SAMPLE 1 ========\n",
      "you , how you stopped feeling and now you me a constant reminder that even the most innocuous mistake is still very much . on , keep going ! . . you a risk being undone . yeah , that about ends well , just do it , say it , say it again , because do want do it , , yes - times [ chorus ] \" i plea you , you for , and i believe you in some other , very same , i plea you , you and i that you me not for like all tha knows what else to do , so that i may go [ of chorus ] \" i said , , \" , just do if you it means that you me not alone ? you me probably be the one out the sun on Sunday ,gh - - adjusting lights , so the twilight a bit more , a bit more on , let that finally ' ' ' go , - having nature ' isoType ' . a very [ verse  62 ] ' , say again , how are you have ever really ' reached the limit ? me , is this really on my radar , might have hidden if i not overtreated you . if this plan ' t work toast , oh , you would die . if this plan ' t work , your toast was the only thing that stood where it did , you probably die . if this plan fail , oh , then i want you to always on the look for thewitch . . . in other words , not on me . here . . . last night , with [ chorus ] . how we got our hair long , it turned ugly when never comes out the side curtain , and on astro-cycling we go to our e - c hair loss prevention and clothing forward - information technology . then we would give any man his or her on me [ 1969 ] or my abilities as a . visual effects - , i the red light . here , , yes i that one time , blocking the street . a silver lining - visible only in the . st [ ] may the slender , . . capable and change a person a color an hour . how , , another idea [ part ] \" i, who do you draw the 12 . \" red , in the red light reading my red . the early [ chorus ] , a woman , getting heavier . she probably was but she a while , new tradition for her not to always exactly the mirror image she always did . her other hand there , on her other , still had it in her that she [ / chorus ] \" a woman who john ' and drinks fine whiskey and who calls the police . \" i forget who she is . . . calls the house , but shes never been kind to anybody . [ ] we also know who she really is , she just is . who of the many , different races and incomes . shes a good girl with a bad idea , but at the end of the day , ' always me an \" and \" goodbye \" goodbye \" . \" [ from verse ] \" i told you , you ' re so psyched up in fact you ' should know , that i know . city , yes . . . in good hands saying well , \" \" i . you should too , ' because here really is something for all . [ groan ] \" . but , city , i . . . want love , that is immediate , no matter the . \" [ and number 2 ] , i just wanna be you with every fibre of my being isn ' t that how it feels like i really was always somewhere you have always been , you have become likeuo your new home [ covering ] . there ' s people to be seein at the airport . he also know also that office space might help your asas me , which i , what i who was once . our romance , then , may well be . to sometimes find us a kiss [ ] , kiss me at the same , more , again . the city , 2011 . o [ 100 ] wanted to be yours truly was mine only your sexy little , all sunshine , little a ' n . yours truly be mine only yours truly be mine only yours truly be mine your own . and mine yours truly be yours truly be mine only yours truly be mine [ . . . ] \" wanna . . . yours truly be yours truly be yours truly be yours truly be yours truly be mine my only true woman me your fantasy mine mine mine mine mine mine mine mine mine [ spoken ] [ tab 2 ] want to close the loop in your . and my . dream , a ' n \" , . be mine ? \" i i ever wanted to . . . be my . dream . . . . . your mine . my [ . 1 ] want in a funny way . ' min a and now want all you are their woman . . s . . ' em like a . i like you really me only half as [ . 2 ] still with ' s . on a t - alls evening , head the chicks and get the \" but you ? \" continue toish over them , \" you \" and . . . miss you . \" [ verse ]\n",
      "\n",
      "[810 | 1067.07] loss=0.06 avg=0.63\n",
      "[820 | 1079.57] loss=0.07 avg=0.62\n",
      "[830 | 1092.07] loss=0.05 avg=0.61\n",
      "[840 | 1104.55] loss=0.07 avg=0.60\n",
      "[850 | 1117.04] loss=0.05 avg=0.59\n",
      "[860 | 1129.54] loss=0.06 avg=0.58\n",
      "[870 | 1142.04] loss=0.05 avg=0.57\n",
      "[880 | 1154.54] loss=0.06 avg=0.57\n",
      "[890 | 1167.05] loss=0.04 avg=0.56\n",
      "[900 | 1179.55] loss=0.05 avg=0.55\n",
      "[910 | 1192.08] loss=0.05 avg=0.54\n",
      "[920 | 1204.59] loss=0.05 avg=0.53\n",
      "[930 | 1217.10] loss=0.05 avg=0.52\n",
      "[940 | 1229.62] loss=0.05 avg=0.52\n",
      "[950 | 1242.12] loss=0.05 avg=0.51\n",
      "[960 | 1254.62] loss=0.04 avg=0.50\n",
      "[970 | 1267.11] loss=0.04 avg=0.49\n",
      "[980 | 1279.62] loss=0.05 avg=0.49\n",
      "[990 | 1292.13] loss=0.04 avg=0.48\n",
      "[1000 | 1304.63] loss=0.05 avg=0.47\n",
      "Saving checkpoint/run1/model-1000\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name='124M',\n",
    "              steps=1000,\n",
    "              restore_from='fresh',\n",
    "              run_name='run1',\n",
    "              print_every=10,\n",
    "              sample_every=200,\n",
    "              save_every=500\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHdTL8NDbAh3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pel-uBULXO2L",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load a Trained Model Checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8F04-UsHznvh",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "# !pip install -q gpt-2-simple\n",
    "import gpt_2_simple as gpt2\n",
    "from datetime import datetime\n",
    "from google.colab import file\n",
    "gpt2.mount_gdrive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCcx5u7sbPTD",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "-fxL77nvAMAX",
    "outputId": "61639f71-08e5-41db-ac67-e160f0edbac8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/run1/model-1000\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1000\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess, run_name='run1')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "GPT-2 AM Lyric Generator",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
